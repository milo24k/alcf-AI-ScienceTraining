{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milo24k/alcf-AI-ScienceTraining/blob/main/07_AI_Accelerators_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Session 7: AI Accelerators Homework Solution**\n",
        "\n",
        "Theory homework:\n",
        "1. What are the key architectural features that make these systems suitable for AI workloads?\n",
        "\n",
        "2. Identify the primary differences between these AI accelerator systems in terms of their architecture and programming models.\n",
        "\n",
        "3. Based on hands-on sessions, describe a typical workflow for refactoring an AI model to run on one of ALCF's AI testbeds (e.g. SambaNova or Cerebras). What tools or software stacks are typically used in this process?\n",
        "\n",
        "4. Give an example of a project that would benefit from AI accelerators and why?"
      ],
      "metadata": {
        "id": "rq6XMKAy-EX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Solution:**"
      ],
      "metadata": {
        "id": "Q1VMg7SvehZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What are the key architectural features that make these systems suitable for AI workloads?"
      ],
      "metadata": {
        "id": "skulixi_IsXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tailored Hardware:** The respective Hardware platform is designed having into consideration the specifics of AI Computing requirements and load, being able to handle, in the case of AI accelerators, operations seen in the AI realmn like tensor operations and matrix multiplications. As an example we have the Cerebras CS-2 which has a hardware arquitecture which is crucial for handling compute-intensive models that you can find working in AI workloads; it has 850,000 cores optimized for sparse linear algebra,Cerebras CS-2 and similar systems are specifically created for handling deep learning and other AI applications which are very compute-demanding.\n",
        "\n",
        "**Memory Bandwidth and On-Chip Memory:** More efficient and on hand readi to be used data speeds up the process. Having a larger bandwidth and On-Chip memory is critical for accelerating memory-intensive AI workloads. As an example, Cerebras has 40 GB of on-chip memory, and SambaNova showcases a vast memory configuration, helping to reduce latency and energy cost associated considered when ther is data movement.\n",
        "\n",
        "**Scalability and Parallelism:**  Counting with architecture that can be reorganized or configured Spatial and Reconfigurable being flexible to allow for the direct mapping of AI models onto the hardware, helps to enhance the performance and tackle problems otherwise difficult to resolve. An example is Graphcore’s IPU which uses a large number of processing tiles (each having its own core and local memory), allowing Bulk Synchronous Parallelism (BSP). This architecture provides the opportunity for an efficient handling of computation and communication making possible parallel processing and deterministic execution.\n"
      ],
      "metadata": {
        "id": "2H9lnNP5I8rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Identify the primary differences between these AI accelerator systems in terms of their architecture and programming models."
      ],
      "metadata": {
        "id": "BOdYeAbRJ2_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SambaNova's Reconfigurable Dataflow Units:\n",
        "SambaNova is focused on big data tasks. This is thanks to a system of layered memory which allows the efficient flow and dealing with large amounts of data. It's managed by SambaFlow (or it's own management software), and it's compatible with other AI tools that make more efficient data-focused tasks.\n",
        "\n",
        "Cerebras Wafer-Scale Engine:\n",
        "Cerebras arquitecture is composed by a large amount of PU's, being each able to process on it's own (due to independent memory) and independent functionality. This enhances the engine's ability to multitaks, allowing speed and scalability. Cerebras is compatible specially with TensorFlow and PyTorch. This is due to the software that composes the most part of the engine's setup.\n",
        "\n",
        "Graphcore IPU:\n",
        "Graphcore’s IPU has it's own processing unit and memory which are interconnected and in large quantities. A layout like this makes possible to process and then share it's data more efficiently using Bulk Synchronous Parallelism (BLK). Poplar SDK is used by Graphcore in order to manage and balance it's units and make sure all runs accordingly.\n",
        "\n",
        "Groq’s Tensor Streaming Processor:\n",
        "Groq's is designed using  Tensor Streaming Processor, which strives on the delivery of consistent and predictable results. Having this kind of results is crucial when time is an important factor to consider. Groq supports reliable execution, which makes easier to run AI models that need fast and steady processing speeds; all this because of avoiding the complications of dynamic memory.\n",
        "\n"
      ],
      "metadata": {
        "id": "id7CNFtAJ6lG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Based on hands-on sessions, describe a typical workflow for refactoring an AI model to run on one of ALCF's AI testbeds (e.g., SambaNova or Cerebras). What tools or software stacks are typically used in this process?"
      ],
      "metadata": {
        "id": "ahkKUnm3ZzqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to refactor an AI model to run on one of ALCF's AI Testbests, you wold need to start with a framework like PyTorch, tailoring to the hardware in use. Afther the fine tunning phase, software like Cerebras SDK is used to administer and make sure the model runs smoothly in the given hardware. The respective model can still need to be further transformed for optimization. As an example, moving from PyTorch to PopTorch or using SambaFlow for model conversion are common steps in this process.\n",
        "Once all of the previous steps are fulfilled, the model would need to be compiled using Cerebras Graph Compiler and it would then be loaded into the hardware to be processed."
      ],
      "metadata": {
        "id": "nmUfLFVvZ5iS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Give an example of a project that would benefit from AI accelerators and why?"
      ],
      "metadata": {
        "id": "jQBoNUBub9f8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "LNN training for image recognition can benefit from AI Accelerators. This, because ofn the huge quantities of data and intense processing, all of which AI accelerators improve exponentially. SambaNova or Cerebras can run these tasks much more efficiently than the traditionally used CPU architecture (also even GPU), bringing down training time to lower numbers going from weeks to days."
      ],
      "metadata": {
        "id": "fyT6bq1z-Jj8"
      }
    }
  ]
}